{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every notebook starts with the following three lines; they ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all the necessary packages. We are going to work with the fastai V1 library which sits on top of Pytorch 1.0. The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at data \n",
    "\n",
    "We are going to use the Food 101 dataset (https://www.kaggle.com/kmader/food41#1028787.jpg ) which features 101 food categories, with 101'000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. For the purpose of this notebook, we will be classifying amongst 5 different food categories (applie pie, waffles, padthai, bread pudding and ramen) and only using 200 umages per food category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "#print(ws.get_details())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "#Get and download food images from the File Datasets from the workspace\n",
    "#Create directory and choose an appropriate folder name for your labeled images\n",
    "\n",
    "apple_pie_dataset = Dataset.get_by_name(ws, name='apple-pie')\n",
    "apple_pie_dataset.download(target_path='./data/apple-pie', overwrite=False)\n",
    "\n",
    "waffles_dataset = Dataset.get_by_name(ws, name='waffles')\n",
    "waffles_dataset.download(target_path='./data/waffles', overwrite=False)\n",
    "\n",
    "padthai_dataset = Dataset.get_by_name(ws, name='pad-thai')\n",
    "padthai_dataset.download(target_path='./data/padthai', overwrite=False)\n",
    "\n",
    "breadpudding_dataset = Dataset.get_by_name(ws, name='bread-pudding')\n",
    "breadpudding_dataset.download(target_path='./data/breadpudding', overwrite=False)\n",
    "\n",
    "ramen_dataset = Dataset.get_by_name(ws, name='ramen')\n",
    "ramen_dataset.download(target_path='./data/ramen', overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an appropriate name for your labeled images. You can run these steps multiple times to create different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "#directories to images \n",
    "path = Path('data')\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for each category to classify\n",
    "classes = ['apple-pie','breadpudding','padthai', 'ramen', 'waffles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any images that cannot be opened \n",
    "for c in classes:\n",
    "    print(c)\n",
    "    verify_images(path/c, delete=True, max_size=2200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Data\n",
    "\n",
    "ImageDataBunch returns a data bunch object which is needed for modeling in fast AI. This contains the images and labels for the training, validation and/or test datasets. \n",
    "\n",
    "We want to make all images the same shape and size for better GPU performance(GPUs have to provide the same instruction to a bunch of things at the same time in order to be fast). We will use size = 224 to ensure that all the images are of 224x224 square size.\n",
    "\n",
    "We use NumPy random seed when we need to generate pseudo-random numbers in a repeatable way*.\n",
    "\n",
    "If we the data is not normalized, we can have difficulty training a model. We want the red, green and blue channels of the images to have a mean of 0 and standard deviation of 1. \n",
    "\n",
    "The get_transforms function makes the size of the image 224 x 224 by doing a combination of things like center cropping and resizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = ImageDataBunch.from_folder(path, train=\".\", valid_pct=0.2,\n",
    "        ds_tfms=get_transforms(), size=224).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a look at our delicious food! Yummm....\n",
    "data.show_batch(rows=5, figsize=(7,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "Convolutional Neural Networks (http://cs231n.github.io/convolutional-networks/) are very similar to ordinary Neural Networks but the assumption is that the inputs are images- they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. Activation functions are applied to each node to capture non linearities in the data.\n",
    "\n",
    "We will use a convolutional neural network backbone and a fully connected head with a single hidden layer as a classifier. We are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 5 outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model name is learn\n",
    "learn = cnn_learner(data, models.resnet34, metrics=[error_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, Run\n",
    "\n",
    "#create new aml experiment \n",
    "experiment = Experiment(ws, 'fastai-food-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "from fastai.vision import (ImageDataBunch, get_transforms, cnn_learner, models, imagenet_stats, accuracy)\n",
    "#from fastai.metrics import error_rate\n",
    "from pathlib import Path \n",
    "from azureml.core.run import Run \n",
    "import numpy as np\n",
    "\n",
    "# get the Azure ML run object\n",
    "run = Run.get_context()\n",
    "\n",
    "# get images\n",
    "path = Path('data')\n",
    "np.random.seed(2)\n",
    "data = ImageDataBunch.from_folder(path,\n",
    "                                       train=\".\",\n",
    "                                       valid_pct=0.2,\n",
    "                                       ds_tfms=get_transforms(),\n",
    "                                       size=224).normalize(imagenet_stats)\n",
    "# build estimator based on ResNet 34\n",
    "learn = cnn_learner(data, models.resnet34, metrics=accuracy)\n",
    "learn.fit_one_cycle(2)\n",
    "# do test time augmentation and get accuracy\n",
    "acc = accuracy(*learn.TTA())\n",
    "# log the accuracy to run`\n",
    "run.log('Accuracy', np.float(acc))\n",
    "print(\"Accuracy: \", np.float(acc))\n",
    "# save model\n",
    "learn.path = Path(\"./outputs\")\n",
    "learn.export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import CondaDependencies\n",
    "from azureml.core import Environment\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "myenv = Environment(name=\"myenv\")\n",
    "conda_dep = CondaDependencies()\n",
    "\n",
    "\n",
    "conda_dep.add_pip_package(\"fastai\")\n",
    "conda_dep.add_pip_package(\"ipykernel\")\n",
    "myenv.python.conda_dependencies=conda_dep\n",
    "\n",
    "azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 50000000000\n",
    "#To submit a run, create a run configuration that combines the script file and environment, and pass it to Experiment.submit. \n",
    "#In this example, the script is submitted to local computer, but you can specify other compute targets such as remote clusters as well.\n",
    "\n",
    "\n",
    "#instead of managing the setup of the environment yourself, you can ask the system to build a new conda environment for you. \n",
    "#The environment is built once, and will be reused in subsequent executions as long as the conda dependencies remain unchanged.\n",
    "\n",
    "myenv.python.user_managed_dependencies = False\n",
    "runconfig = ScriptRunConfig(source_directory=\".\", script=\"train.py\")\n",
    "runconfig.run_config.target = \"local\"\n",
    "runconfig.run_config.environment = myenv\n",
    "run = experiment.submit(config=runconfig)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails \n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#create outputs folder in github repo\n",
    "learn.path = Path(\"./outputs\")\n",
    "learn.export()\n",
    "path = learn.path\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define remote compute target to use\n",
    "# Further docs on Remote Compute Target: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-remote\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"gpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "     found = True\n",
    "     print('Found existing training cluster.')\n",
    "     # Get existing cluster\n",
    "     # Method 1:\n",
    "     aml_remote_compute = cts[amlcompute_cluster_name]\n",
    "     # Method 2:\n",
    "     # aml_remote_compute = ComputeTarget(ws, amlcompute_cluster_name)\n",
    "    \n",
    "if not found:\n",
    "     print('Creating a new training cluster...')\n",
    "     provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D13_V2\", # for GPU, use \"STANDARD_NC12\"\n",
    "                                                                 #vm_priority = 'lowpriority', # optional\n",
    "                                                                 max_nodes = 20)\n",
    "     # Create the cluster.\n",
    "     aml_remote_compute = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "aml_remote_compute.wait_for_completion(show_output = True, min_node_count = 0, timeout_in_minutes = 20)\n",
    "    \n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you would like to run remotely, you can run this cell and the following two cells \n",
    "\n",
    "#Next, we construct an `azureml.train.dnn.PyTorch` estimator object\n",
    "#The PyTorch estimator provides a simple way of launching a PyTorch training job on a compute target. \n",
    "#We'll submit the train.py file using the `entry_script` parameter and use the gpu-cluster we created above as our `compute target`\n",
    "#Since fastai v1 is built on top of PyTorch 1.0, we can use the `framework_version` parameter to specify our PyTorch version, and the estimator will automatically retrieve a Docker image that has PyTorch 1.0 and its dependencies installed.\n",
    "#Finally, on top of our PyTorch base image, we'll add the fastai v1 and pillow packages using the `pip_packages` parameter. The pillow package is necessary for image processing, and we'll pin to versions below 7.0.0 since those are known to be compatible with PyTorch 1.0's torchvision package\n",
    "#pillow's latest version is not compatible with the latest version of fastAI\n",
    "#framework version - we want to install pytorch version 1.0 only works with fastai version 1 \n",
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "estimator = PyTorch(source_directory=\".\",\n",
    "                            compute_target=\"gpu-cluster\",\n",
    "                            entry_script='train.py',\n",
    "                            framework_version='1.0',\n",
    "                            pip_packages=['fastai', 'pillow<7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 500000000000\n",
    "run = experiment.submit(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to the outputs directory for capture. Note: this is not registering model\n",
    "from azureml.core import Experiment\n",
    "import joblib\n",
    "model_file_name = 'outputs/export1.pkl'\n",
    "\n",
    "joblib.dump(value = my_model, filename = model_file_name)\n",
    "\n",
    "# upload the model file explicitly into artifacts \n",
    "run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the model\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# register the model1.pkl file provided in the output folder \n",
    "model = Model.register(model_path = \"outputs/export1.pkl\", # this points to a local file\n",
    "                       model_name = \"food_classification_model\", # this is the name the model is registered as\n",
    "                       tags = {'food': \"Yummmm :)\"},\n",
    "                       description = \"Model predicting types of food\",\n",
    "                       workspace = ws)\n",
    "\n",
    "print(model.name, model.description, model.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to deploy\n",
    "\n",
    "\n",
    "To deploy the model, you need the following items:\n",
    "\n",
    "1) An entry script, this script accepts requests, scores the requests by using the model, and returns the results.\n",
    "2) Dependencies, like helper scripts or Python/Conda packages required to run the entry script or model.\n",
    "3) The deployment configuration for the compute target that hosts the deployed model. This configuration describes things like memory and CPU requirements needed to run the model.\n",
    "\n",
    "1. Define your entry script and dependencies\n",
    "Entry script\n",
    "We will first write the entry script as shown below. Note a few points in the entry script.\n",
    "\n",
    "The script contains two functions that load and run the model:\n",
    "\n",
    "init(): Typically, this function loads the model into a global object. This function is run only once, when the Docker container for your web service is started.\n",
    "\n",
    "When you register a model, you provide a model name that's used for managing the model in the registry. You use this name with the Model.get_model_path() method to retrieve the path of the model file or files on the local file system. If you register a folder or a collection of files, this API returns the path of the directory that contains those files.\n",
    "\n",
    "run(input_data): This function uses the model to predict a value based on the input data. Inputs and outputs of the run typically use JSON for serialization and deserialization. You can also work with raw binary data. You can transform the data before sending it to the model or before returning it to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import json\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Workspace\n",
    "import fastai \n",
    "from fastai.vision import *\n",
    "from fastai.metrics import accuracy \n",
    "\n",
    "global food_classification_model\n",
    "\n",
    "def init():   \n",
    "    model_filename=\"export1.pkl\"\n",
    "    # The AZUREML_MODEL_DIR environment variable indicates a directory containing the model file you registered.  \n",
    "    model_path=os.getenv('AZUREML_MODEL_DIR')\n",
    "    food_classification_model = load_learner(path=model_path, file=model_filename)\n",
    "  \n",
    "    \n",
    "    print(food_classification_model.data.classes)\n",
    "\n",
    "\n",
    "def run(request):\n",
    "    img = open_image(request)\n",
    "    prediction = food_classification_model.predict(img)\n",
    "    pred = str(prediction[0])\n",
    "    print(pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #init()\n",
    "    #request = \"data/apple-pie/3068872.jpg\"\n",
    "    #run(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a tuple of three things: the object predicted (with the class in this instance), the underlying data (here the corresponding index) and the raw probabilities. You can also do inference on a larger set of data by adding a test set. This is done by passing an ItemList to load_learner.\n",
    "\n",
    "learn = load_learner(mnist, test=ImageList.from_folder(mnist/'test'))\n",
    "https://docs.fast.ai/tutorial.inference.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dependencies\n",
    "The following YAML is the Conda dependencies file we will use for inference. If you want to use automatic schema generation, your entry script must import the inference-schema packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile myenv.yml\n",
    "\n",
    "name: project_environment\n",
    "dependencies:\n",
    "- python=3.6.9\n",
    "\n",
    "- pip:\n",
    "  - fastai\n",
    "  - torch\n",
    "  - torchvision  \n",
    "  - azureml-defaults\n",
    "  - azureml-core\n",
    "  - matplotlib==3.1.2\n",
    "- numpy==1.16.2\n",
    "- Pillow==6.2.0\n",
    "- pandas==0.23.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Instantiate environment\n",
    "myenv = Environment.from_conda_specification(name = \"myenv\",\n",
    "                                             file_path = \"myenv.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your inference configuration. The inference configuration describes how to configure the model to make predictions. This configuration isn't part of your entry script. It references your entry script and is used to locate all the resources required by the deployment. It's used later, when you deploy the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your deployment configuration Before deploying your model, you must define the deployment configuration. The deployment configuration is specific to the compute target that will host the web service. The deployment configuration isn't part of your entry script. It's used to define the characteristics of the compute target that will host the model and entry script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags = {'task': \"image-classification\"}, \n",
    "                                               description='A model to predict types of food')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment uses the inference configuration deployment configuration to deploy the models. The deployment process is similar regardless of the compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Model.deploy(ws, name='image-classification-fastai14', models=[model], inference_config= inference_config, deployment_config=aciconfig)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"data/apple-pie/3068872.jpg\"\n",
    "prediction = service.run(request)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(Image.open('data/apple-pie/3068872.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test our deployed web service. We will send the data as a JSON string to the web service hosted in ACI and use the SDK's run API to invoke the service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "    \n",
    "def preprocess(image_file):\n",
    "    \"\"\"Preprocess the input image.\"\"\"\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_file)\n",
    "    image = data_transforms(image).float()\n",
    "    image = torch.tensor(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "input_data = preprocess('data/apple-pie/3068872.jpg')\n",
    "\n",
    "lists = input_data.tolist()\n",
    "\n",
    "#convert input data into JSON string \n",
    "json_str = json.dumps(lists)\n",
    "print(type(json_str))\n",
    "# deserializes sample to a python object \n",
    "sample = json.loads(json_str)\n",
    "print(type(sample))\n",
    "# serializes sample to JSON formatted string as expected by the scoring script\n",
    "sample = json.dumps({\"input_data\":sample})\n",
    "print(type(sample))\n",
    "#prediction = service.run(sample)\n",
    "#print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"data/apple-pie/3068872.jpg\"\n",
    "sample = json.dumps({\"request\":sample})\n",
    "print(sample)\n",
    "print(type(sample))\n",
    "\n",
    "prediction = service.run(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "global food_classification_model\n",
    "\n",
    "def init():\n",
    "    path = learn.path\n",
    "    path=os.getenv(('AZUREML_MODEL_DIR'))\n",
    "    filename=\"export1.pkl\"\n",
    "    food_classification_model = load_learner(path=path, file=filename)\n",
    "    print(food_classification_model.data.classes)\n",
    "\n",
    "\n",
    "def run(request):\n",
    "    img = open_image(request)\n",
    "    prediction = food_classification_model.predict(img)\n",
    "    pred = str(prediction[0])\n",
    "    print(pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #init()\n",
    "    #request = \"data/apple-pie/3068872.jpg\"\n",
    "    #run(request)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"data/apple-pie/3068872.jpg\"\n",
    "run(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"data/apple-pie/3068872.jpg\"\n",
    "prediction = service.run(request)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = learn.path\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    # note here \"food_classification_model\" is the name of the model registered under the workspace, this call should return the path to the model.pkl file on the local disk\n",
    "    path = os.getenv('myenv')\n",
    "    filename=\"export1.pkl\"\n",
    "    food_classification_model = load_learner(path=path, file=filename)\n",
    "    \n",
    "    print(food_classification_model.classes)    \n",
    "    \n",
    "    \n",
    "def run(request):\n",
    "    img = open_image(request)\n",
    "    prediction = food_classification_model.predict(img)\n",
    "    print(prediction)\n",
    "    return str(prediction[0])\n",
    "\n",
    "sample = \"data/apple-pie/3068872.jpg\"\n",
    "sample = json.dumps({\"request\":sample})\n",
    "print(sample)\n",
    "print(type(sample))\n",
    "\n",
    "#prediction = service.run(sample)\n",
    "#print(prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring using fast AI \n",
    "img = open_image(path/'apple-pie'/'3068872.jpg')\n",
    "img\n",
    "pred_class,pred_idx,outputs = learn.predict(img)\n",
    "pred_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
